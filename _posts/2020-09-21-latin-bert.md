---
title: "Latin BERT: A Contextual Language Model for Classical Philology"
layout: paper
date: 2020-09-21 00:00
tag: paper
image:
headerImage: false
projects: false
hidden: true # don't count this post in blog pagination
description: "Preprint on arXiv"
jemoji: '<img class="emoji" title=":paper:" alt=":paper:" src="../assets/images/paper-icon.png" height="20" width="20" align="absmiddle">'
author: patrickburns
externalLink: false
---

# Latin BERT: A Contextual Language Model for Classical Philology
Co-written with David Bamman.  
*Preprint available at  [arXiv:2009.10053 \[cs.CL\]](https://arxiv.org/abs/2009.10053)*

{:.paper-section-heading}
## Abstract
We present Latin BERT, a contextual language model for the Latin language, trained on 642.7 million words from a variety of sources spanning the Classical era to the 21st century. In a series of case studies, we illustrate the affordances of this language-specific model both for work in natural language processing for Latin and in using computational methods for traditional scholarship: we show that Latin BERT achieves a new state of the art for part-of-speech tagging on all three Universal Dependency datasets for Latin and can be used for predicting missing text (including critical emendations); we create a new dataset for assessing word sense disambiguation for Latin and demonstrate that Latin BERT outperforms static word embeddings; and we show that it can be used for semantically-informed search by querying contextual nearest neighbors. We publicly release trained models to help drive future work in this space.  

{:.paper-section-heading}
## Citation
Bamman, D., and Burns, P.J. 2020. “Latin BERT: A Contextual Language Model for Classical Philology.” arXiv:2009.10053 [Cs] (September 21). http://arxiv.org/abs/2009.10053.  
